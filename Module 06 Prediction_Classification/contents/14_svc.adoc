= SVC(Support Vector Classifier)

SVC는 **서포트 벡터 머신(SVM, Support Vector Machine)**의 분류 문제를 해결하기 위한 구현 중 하나로, Scikit-learn 라이브러리에서 제공하는 클래스입니다. SVC는 주어진 데이터를 가장 잘 분리하는 **최적의 초평면(Hyperplane)**을 찾는 분류 알고리즘입니다.

== 개념

1. 초평면 (Hyperplane)
* 데이터 공간에서 두 개 이상의 클래스를 나누는 결정 경계(decision boundary)를 의미합니다.
* 예를 들어, 2차원 공간에서는 직선, 3차원에서는 평면이 됩니다.
2. 마진 (Margin)
* 초평면과 가장 가까운 데이터 포인트(서포트 벡터) 사이의 거리입니다.
* SVC는 마진을 최대화하는 초평면을 찾아 데이터 분류 성능을 높입니다.
3. 서포트 벡터 (Support Vectors):
* 초평면에 가장 가까운 데이터 포인트들로, 결정 경계를 형성하는 데 중요한 역할을 합니다.
* 모델은 이 서포트 벡터들에만 영향을 받습니다.

== 원리

1. 선형 분류
* SVC는 먼저 데이터를 선형적으로 분리 가능한 경우 최적의 초평면을 찾습니다.
* 초평면 방정식 +
𝑤 ⋅ 𝑥 + 𝑏 = 0

* 𝑤: 초평면의 기울기
* 𝑏: 절편
2. 비선형 분류
* 데이터가 선형적으로 분리되지 않는 경우 **커널 함수(Kernel Function)**를 사용하여 고차원 공간으로 데이터를 매핑합니다.
* 고차원 공간에서 데이터를 선형적으로 분리한 후, 다시 원래 공간으로 변환합니다.
3. 목표
* 마진을 최대화하면서 오분류를 최소화하는 초평면을 찾습니다.
* 손실 함수(Loss Function)와 정규화 항을 통해 균형을 조정합니다.

== 커널 함수

커널 함수는 데이터를 고차원 공간으로 변환하여 선형 분리가 가능하도록 합니다. SVC에서 제공하는 커널 함수에는 다음과 같은 종류가 있습니다:

1. 선형 커널 (Linear Kernel):
* 선형적으로 분리 가능한 데이터에 적합.
* 계산 속도가 빠르고 간단함.
2. 다항식 커널 (Polynomial Kernel):
* 다항식 차수를 기반으로 고차원 특징을 생성.
* 복잡한 데이터에 적합.
3. RBF 커널 (Radial Basis Function Kernel, 가우시안 커널):
* 비선형 데이터에 자주 사용.
* 데이터 포인트 간의 거리 기반으로 작동.
4. 시그모이드 커널 (Sigmoid Kernel):
* 신경망의 활성화 함수처럼 작동.

== 주요 하이퍼파라미터

1. C (Regularization Parameter)
* 마진의 크기와 오분류 허용 간의 균형을 조정.
* 값이 작을수록 마진이 커지고, 값이 클수록 오분류를 줄이기 위해 초평면이 데이터에 민감해짐(과적합 가능).
2. 커널 (Kernel):
*. 데이터 특성과 분포에 따라 적합한 커널을 선택.
3. 감마 (Gamma):
* RBF 커널에서 데이터 포인트의 영향 범위를 조정.
* 값이 작으면 더 넓은 범위에 영향을 주고, 값이 크면 좁은 범위에만 영향을 미침.

== 장점

1. 고차원 데이터 처리 가능:
* 고차원 공간에서도 효과적으로 작동하며, 초평면을 기반으로 데이터를 분리.
2. 효율적인 메모리 사용:
* 서포트 벡터만 저장하므로 메모리 효율적.
3. 비선형 분류 가능:
* 커널 함수를 통해 복잡한 비선형 데이터를 처리할 수 있음.
4. 과적합 방지:
* 정규화(C 파라미터)를 통해 과적합을 방지.

== 단점
1. 큰 데이터셋에서 비효율적
* 데이터 크기가 커지면 계산 비용이 급격히 증가.
2. 하이퍼파라미터 튜닝 필요
* 최적의 성능을 위해 C, 감마 등의 값을 세심하게 조정해야 함.
3. 해석 어려움
* 특히 비선형 커널을 사용할 경우, 결과 해석이 어려울 수 있음.

== 사용 사례
1. 이미지 분류:
* 예: 손글씨 숫자 분류(MNIST 데이터셋).
2. 문서 분류 및 텍스트 마이닝:
* 예: 스팸 이메일 분류.
3. 생물정보학:
* 예: 유전자 데이터 분석.
4. 의료 데이터 분석:
* 예: 질병 예측 및 분류.