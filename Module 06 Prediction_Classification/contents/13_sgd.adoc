= 확률적 경사하강법(Stochastic Gradient Decent- SGD)

확률적 경사하강법(SGD)은 머신러닝에서 최적화 알고리즘 중 하나로, 주어진 손실 함수의 최소값(또는 최대값)을 찾기 위해 반복적으로 가중치를 업데이트하는 방법입니다. 확률적이라는 용어는 전체 데이터셋 대신 랜덤하게 선택된 데이터 포인트를 사용하여 경사를 계산한다는 것을 의미합니다.

== 경사하강법의 종류

경사하강법에는 다음 세 가지 주요 변형이 있습니다:

1. 배치 경사하강법 (Batch Gradient Descent)
* 데이터셋 전체를 사용하여 경사를 계산.
* 한 번의 업데이트에 많은 계산 자원이 필요.
* 데이터셋이 클수록 느려질 수 있음.
2. 확률적 경사하강법 (SGD)
* 데이터셋에서 무작위로 하나의 데이터 포인트를 선택하여 경사를 계산.
* 더 빠르게 업데이트 가능하지만, 노이즈로 인해 수렴 과정이 불안정할 수 있음.
3. 미니배치 경사하강법 (Mini-batch Gradient Descent)
* 데이터셋을 작은 배치로 나누고 각 배치에 대해 경사를 계산.
* 배치 경사하강법과 확률적 경사하강법의 절충안.

== 특징

1. 작동 방식
* SGD는 매 반복(iteration)마다 하나의 데이터 포인트 (𝑥~𝑖~,𝑦~𝑖~))를 랜덤하게 선택하여 경사를 계산하고 가중치를 업데이트합니다. +
𝜃 := 𝜃 − 𝜂 ⋅ ∇~𝜃~𝐿(𝜃;𝑥~𝑖~,𝑦~𝑖~)
** 𝜃: 가중치 벡터(모델 파라미터)
** 𝜂: 학습률 (learning rate)
** ∇~𝜃~𝐿(𝜃;𝑥~𝑖~,𝑦~𝑖~): 선택된 데이터 포인트에 대한 손실 함수의 기울기
2. 장점
* 빠른 업데이트: 데이터셋 전체를 사용할 필요가 없으므로 업데이트 속도가 빠름.
* 온라인 학습 가능: 실시간으로 들어오는 데이터를 학습 가능.
* 지역 최적값 탈출 가능성: 경사가 매번 달라져 수렴 과정에서 노이즈가 발생하지만, 이는 복잡한 손실 함수에서 지역 최적값(local minima)을 탈출하는 데 도움이 될 수 있음.
3. 단점
* 수렴 불안정: 경사에 노이즈가 많아 최적점에 도달하지 못하거나 수렴 속도가 느릴 수 있음.
* 학습률 선택이 중요: 너무 큰 학습률은 발산을 초래하고, 너무 작은 학습률은 학습을 느리게 함.

== 활용

확률적 경사하강법은 머신러닝과 딥러닝 모델의 학습 과정에서 널리 사용됩니다:

* 선형 회귀 및 로지스틱 회귀 +
파라미터 최적화를 위한 간단한 SGD 적용.
* 신경망 학습 +
딥러닝에서 매우 큰 데이터셋을 처리할 때 필수적.
* 온라인 학습 +
데이터가 실시간으로 들어오는 환경에서 사용 (예: 추천 시스템, 실시간 광고).

== 요약

* 데이터셋에서 일부 데이터만 사용하여 계산량 감소.
* 빠른 업데이트로 대규모 데이터셋에 적합.
* 노이즈로 인해 수렴이 불안정하지만, 적절한 하이퍼파라미터 설정(학습률, 모멘텀 등)으로 극복 가능.
* 다양한 최적화 알고리즘(Adam, RMSprop 등)의 기반이 됨.

SGD는 경사하강법의 핵심 변형으로, 데이터 규모가 크거나 연산 효율성이 중요한 경우 널리 사용됩니다.