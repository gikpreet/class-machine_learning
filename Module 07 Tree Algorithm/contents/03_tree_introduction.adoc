= Tree 알고리즘 개념

== 트리 알고리즘의 기본 개념

트리 알고리즘은 다음과 같은 구조로 이루어집니다:

* 노드(Node): 데이터를 처리하거나 결과를 나타내는 지점.
** 루트 노드(Root Node): 트리의 시작점.
** 내부 노드(Internal Node): 데이터를 분할하는 기준이 정의된 노드.
** 단말 노드(Leaf Node): 최종 결과(예측값 또는 클래스)가 포함된 노드.
* 가지(Branch): 노드 간의 연결. 분할 기준에 따라 데이터를 이동시키는 경로.
+
트리 알고리즘은 데이터를 반복적으로 분할하여 특정 기준에 따라 트리 구조를 생성합니다.

== 트리 알고리즘의 종류
1. 결정 트리(Decision Tree)
* 데이터를 분류(Classification)하거나 연속 값을 예측(Regression)하는 데 사용됩니다.
* 특정 기준(정보 이득, 지니 계수 등)에 따라 데이터를 분할합니다.
* 예: 스팸 이메일 분류, 환자의 질병 진단 등.
2. 랜덤 포레스트(Random Forest)
* 다수의 결정 트리를 훈련하고, 이를 앙상블로 결합한 알고리즘입니다.
* 각 트리는 독립적으로 학습하며, 최종 결과는 다수결(분류) 또는 평균(회귀)으로 도출됩니다.
* 장점: 과적합 방지, 높은 예측 성능.
3. 그래디언트 부스팅(Gradient Boosting)
* 트리 기반 모델을 순차적으로 학습하여, 이전 모델의 오류를 보완하는 방식으로 동작합니다.
* 대표 알고리즘:
** XGBoost
** LightGBM
** CatBoost
* 장점: 높은 예측 성능, 튜닝 가능성.
4. 의사 결정 회귀 트리(Regression Tree)
* 종속 변수(타깃)가 연속형 데이터일 때 사용됩니다.
* 데이터를 분할한 후, 각 단말 노드의 평균값을 예측값으로 사용합니다.
5. 분류 및 회귀 트리(CART, Classification and Regression Tree)
* 결정 트리 알고리즘의 한 종류로, 분류와 회귀를 모두 지원합니다.
* 이진 분할(Binary Split)을 기반으로 데이터를 처리합니다.

== 트리 알고리즘의 특징
1. 장점
* 직관적 해석: 트리 구조는 규칙 기반이므로 사람이 이해하기 쉽습니다.
* 비선형 데이터 처리: 데이터의 복잡한 관계를 잘 학습합니다.
* 전처리 요구가 낮음: 데이터의 정규화나 스케일링이 필요하지 않습니다.
* 다중 클래스 문제 처리 가능: 다수의 클래스가 있는 문제에도 효과적입니다.
2. 단점
* 과적합: 트리가 너무 깊어지면 학습 데이터에 지나치게 적합하여 일반화 성능이 저하됩니다.
* 불안정성: 데이터의 작은 변화에도 트리 구조가 크게 변할 수 있습니다.
* 계산 복잡도: 데이터가 크거나 특성이 많을 경우 학습 시간이 오래 걸릴 수 있습니다.

== 트리 알고리즘의 작동 원리

트리 알고리즘은 데이터 분할을 반복적으로 수행하여 트리를 구축합니다.

1. 특성 선택 +
각 특성에 대해 데이터를 얼마나 잘 분할할 수 있는지 평가합니다.
2. 분할 기준:
* 정보 이득(Information Gain): 엔트로피 감소량.
* 지니 계수(Gini Index): 불순도 측정.
* 분산 감소(Variance Reduction): 회귀 문제에서 주로 사용.
3. 트리 성장: 데이터를 반복적으로 분할하여 트리의 깊이를 늘려갑니다.
4. 멈춤 조건: 트리의 깊이가 최대 한도에 도달하거나, 더 이상 유의미한 분할이 없을 때 멈춥니다.

== 활용

* 분류
** 의료 데이터에서 질병 진단.
** 고객의 이탈 여부 예측.
* 회귀
** 주택 가격 예측.
** 판매량 예측.
* 특성 중요도 평가
** 특성 선택(feature selection) 과정에서 유용.

트리 알고리즘은 단순하지만 강력하며, 확장된 알고리즘(Random Forest, Gradient Boosting 등)과 결합하면 더욱 높은 성능을 발휘합니다. 데이터에 적합한 트리 알고리즘을 선택하여 적용하면 다양한 문제를 효과적으로 해결할 수 있습니다.