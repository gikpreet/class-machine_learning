= 앙상블 러닝 (Ensemble Learning)

앙상블 러닝은 **여러 개의 모델(기본 학습기 또는 약한 학습기)**을 결합하여 개별 모델보다 더 나은 성능을 얻는 기법입니다. 개별 모델이 가지는 약점을 보완하고 강점을 강화하여 일관되고 정확한 예측을 목표로 합니다.

== 앙상블 러닝의 핵심 개념

1. 기본 학습기 (Base Learner):
* 개별적으로 학습되는 모델로, 약한 학습기(Weak Learner)나 강한 학습기(Strong Learner)일 수 있습니다.
* 예: 결정 트리, 로지스틱 회귀, 서포트 벡터 머신 등.
2. 앙상블 방법의 목표:
* 여러 모델의 결과를 결합하여 오류를 줄이고 예측 성능을 개선.

3. 다양성 (Diversity):
* 앙상블 모델이 효과적이려면 기본 학습기들이 서로 상관성이 낮아야 합니다.
* 서로 다른 알고리즘, 데이터 샘플링, 하이퍼파라미터 설정 등을 통해 다양성을 확보.

== 앙상블 러닝의 주요 유형

앙상블 러닝은 기본 학습기의 결합 방식에 따라 크게 세 가지로 나뉩니다:

1. 배깅 (Bagging, Bootstrap Aggregating):
* 기본 학습기들이 독립적으로 학습하고, 결과를 평균(회귀) 또는 다수결(분류)로 결합.
* 데이터의 다양한 부분을 활용하기 위해 부트스트래핑(중복 허용 샘플링) 기법 사용.
*  대표 알고리즘
** 랜덤 포레스트(Random Forest): 여러 결정 트리를 결합한 배깅 기법.
* 특징:
** 과적합 방지에 효과적.
** 분산(Variance)을 줄이는 데 중점.

2. 부스팅 (Boosting):
* 기본 학습기를 순차적으로 학습시키며, 이전 모델이 잘못 예측한 데이터에 가중치를 부여.
* 각 학습기는 이전 모델의 약점을 보완.
* 대표 알고리즘
** AdaBoost +
이전 학습기의 오류에 가중치를 부여하는 간단한 부스팅 기법.
** radient Boosting +
손실 함수를 점진적으로 최적화하는 방식.
** XGBoost, LightGBM, CatBoost +
Gradient Boosting의 개선된 버전.
* 특징
** 바이어스 감소에 효과적.
** 과적합 가능성이 있으므로 정규화 기법 필요.

3. 스태킹 (Stacking):

* 서로 다른 종류의 기본 학습기들을 조합하여 최종 결과를 예측.
* 메타 학습기(Meta Learner)를 사용하여 기본 학습기의 예측 결과를 결합.
* 특징
** 다양한 모델을 결합하여 더 높은 성능을 도모.
** 적절한 메타 학습기 선택이 중요.

==  앙상블 러닝의 장점
1. 성능 향상
* 단일 모델보다 일반적으로 더 높은 정확도를 제공.
2. 안정성
* 데이터 노이즈에 덜 민감하고, 과적합 가능성을 줄임.
3. 다양한 문제 해결
* 분류(Classification), 회귀(Regression), 이상 탐지(Anomaly Detection) 등 다양한 문제에 적용 가능.