= 과대 적합(Overfitting)

**과대적합(Overfitting)**은 기계 학습에서 모델이 훈련 데이터에 너무 잘 맞춰져서 일반화 능력이 떨어지는 현상입니다. 즉, 모델이 훈련 데이터에 포함된 노이즈나 불필요한 세부 사항까지 학습하여, 새로운 데이터나 실제 상황에서는 잘 작동하지 않게 되는 경우를 말합니다. 과대적합은 모델의 복잡도가 너무 높거나 훈련 데이터에 너무 잘 맞는 경우에 발생합니다.

== 과대적합의 원인

과대적합은 여러 가지 원인으로 발생할 수 있습니다:

1. 모델의 복잡도 +
모델이 너무 복잡하면 훈련 데이터의 모든 세부 사항을 학습하려고 시도합니다. 예를 들어, 너무 많은 특성(변수)이나 과도하게 깊은 신경망을 사용하는 경우 모델이 훈련 데이터의 노이즈까지 기억하게 될 수 있습니다.
2. 훈련 데이터 부족 +
훈련 데이터가 적으면 모델이 일반화 능력을 가지기 어려워지며, 훈련 데이터에 과도하게 맞춰지기 쉽습니다. 작은 데이터셋에서는 모델이 지나치게 특정 패턴을 학습할 수 있습니다.
3. 특성의 다중공선성(Multicollinearity) +
데이터의 특성들 간에 상관관계가 매우 높은 경우, 모델이 특성들 간의 미세한 차이도 과도하게 학습하면서 과대적합이 발생할 수 있습니다.
4. 노이즈나 이상치 +
훈련 데이터에 포함된 노이즈나 이상치(outliers)는 모델이 이를 학습하려고 하면서 과대적합을 일으킬 수 있습니다.

== 과대적합의 징후
과대적합이 발생하면, 훈련 데이터에서는 매우 좋은 성능을 보이지만, 검증 데이터나 테스트 데이터에서는 성능이 급격히 떨어지게 됩니다. 이는 모델이 훈련 데이터에만 최적화되어, 새로운 데이터에는 일반화되지 않음을 의미합니다.

* 훈련 오류는 낮고, 검증 오류나 테스트 오류는 상대적으로 높습니다.
* 훈련 데이터에 대해 매우 높은 정확도를 보이지만, 검증 데이터에 대해서는 정확도가 급격히 떨어지는 경우가 많습니다.

== 과대적합 회피 기법
과대적합을 방지하거나 이를 완화하기 위해 사용할 수 있는 여러 기법이 있습니다:

1. 데이터 확보
훈련 데이터가 적으면 모델이 훈련 데이터에 과도하게 맞춰지기 쉽습니다. 따라서 데이터를 추가하거나 데이터 증강 기법을 사용하여 훈련 데이터를 늘리는 것이 중요합니다.
2. 모델 단순화하기
모델이 너무 복잡하면 과대적합이 발생할 가능성이 높습니다. 따라서 모델의 복잡도를 줄이는 것이 중요합니다. 예를 들어, 신경망의 층 수를 줄이거나, 결정 트리의 깊이를 제한하는 방법이 있습니다.
* 과적합을 방지하는 하이퍼파라미터 튜닝 +
결정 트리의 최대 깊이나 신경망의 은닉층 수를 제한하여 모델을 단순화할 수 있습니다.
3. 교차 검증(Cross-validation)
교차 검증은 데이터를 여러 번 나누어 모델을 훈련시키고 검증하는 방법입니다. 이를 통해 훈련 데이터에 과대적합되지 않도록 할 수 있습니다. 일반적으로 K-폴드 교차 검증을 사용합니다.
4. 정규화(Regularization)
정규화는 모델이 과도하게 복잡해지는 것을 방지하는 기법입니다. 정규화는 모델에 패널티를 부여하여 큰 가중치를 갖는 특성들이 과도하게 중요해지는 것을 방지합니다.
* L1 정규화(Lasso) +
가중치의 절댓값의 합을 최소화하여 일부 특성을 제거하거나 축소시킵니다.
* L2 정규화(Ridge) +
가중치의 제곱합을 최소화하여 모델을 더 일반화합니다.
+
정규화는 과대적합을 방지하는 데 매우 중요한 역할을 합니다.
5. 드롭아웃(Dropout)
드롭아웃은 신경망에서 사용되는 기법으로, 학습 중에 임의로 일부 뉴런을 제외하여 네트워크가 특정 뉴런에 과도하게 의존하지 않도록 하는 방법입니다. 이는 모델이 더 강건하게 학습될 수 있도록 도와줍니다.
6. 앙상블 기법(Ensemble Methods)
앙상블 기법은 여러 모델을 결합하여 예측 성능을 높이는 방법입니다. 앙상블 기법을 사용하면 과대적합을 줄이고, 모델의 예측을 더 일반화할 수 있습니다. 대표적인 앙상블 기법으로는 배깅(Bagging), 부스팅(Boosting), 랜덤 포레스트(Random Forest), 그라디언트 부스팅(Gradient Boosting) 등이 있습니다.
7. Early Stopping
딥러닝 모델에서 Early Stopping은 훈련 중에 검증 데이터의 성능이 더 이상 향상되지 않으면 훈련을 멈추는 기법입니다. 이를 통해 모델이 과도하게 훈련되는 것을 방지하고, 최적의 시점에서 훈련을 종료할 수 있습니다.
8. 배치 정규화(Batch Normalization)
배치 정규화는 각 층의 출력을 정규화하여 학습을 안정시키고, 과대적합을 방지하는 기법입니다. 이는 모델이 더 빠르고 안정적으로 학습할 수 있도록 도와줍니다.

== 과대적합과 과소적합의 균형

과대적합은 훈련 데이터에 비해 모델이 너무 복잡하여 새로운 데이터에 잘 일반화되지 않는 문제인 반면, **과소적합(Underfitting)**은 모델이 너무 간단하여 훈련 데이터 자체도 잘 학습하지 못하는 현상입니다. 따라서 모델을 훈련할 때는 과대적합과 과소적합의 균형을 맞추는 것이 중요합니다.

* 과대적합
모델이 훈련 데이터의 노이즈나 세부 사항까지 학습하여 새로운 데이터에 잘 적용되지 않음.
* 과소적합
모델이 너무 단순하여 훈련 데이터에서조차 잘 학습되지 않음.

과대적합은 기계 학습 모델이 훈련 데이터에 과도하게 맞춰져서 일반화 능력이 떨어지는 문제입니다. 이를 방지하려면 모델의 복잡도를 적절히 조절하고, 데이터를 충분히 확보하며, 정규화, 교차 검증, 드롭아웃 등의 기법을 활용하여 일반화 성능을 높이는 것이 중요합니다. 과대적합과 과소적합 사이에서 균형을 맞추는 것이 성공적인 모델 학습의 핵심입니다.