= 차원 축소

**차원 축소(Dimensionality Reduction)**의 원리는 고차원 데이터를 저차원으로 변환하면서 중요한 정보를 최대한 보존하는 데 있습니다. 차원 축소는 주로 데이터의 구조, 변동성, 패턴을 유지하면서 데이터의 특성(변수) 수를 줄이는 데 초점을 둡니다.

== 차원 축소의 필요성

. 고차원의 문제:
* 데이터가 고차원일수록 분석 및 모델 학습에 어려움이 발생합니다. 이를 "차원의 저주(Curse of Dimensionality)"라고 합니다.
* 고차원의 데이터는 계산 비용 증가, 과적합 위험 증가, 시각화 어려움을 초래합니다.
. 효율적인 데이터 표현:
* 많은 특성이 중복되거나 관련성이 낮을 수 있습니다. 이러한 특성을 제거하고 데이터를 단순화합니다.
. 노이즈 제거:
* 고차원 데이터의 일부 특성은 노이즈일 가능성이 큽니다. 이를 제거하여 데이터의 품질을 향상시킵니다.

== 차원 축소의 원리

차원 축소는 데이터를 저차원 공간으로 매핑하면서 중요한 정보를 최대한 유지하려고 합니다. 대표적인 원리는 다음과 같습니다:

.  데이터 변동성 최대화
* 데이터의 분산(variance)이 큰 방향은 데이터의 중요한 구조를 나타냅니다.
* 차원 축소는 이러한 변동성을 최대한 유지하려는 방향으로 데이터를 투영합니다.
.  데이터 간 거리 및 구조 보존
* 데이터 포인트 간의 상대적인 거리나 패턴을 보존하여 축소된 공간에서도 유사성을 유지합니다.
.  불필요한 정보 제거
* 중복된 특성이나 기여도가 낮은 특성을 제거하여 데이터를 단순화합니다.

== 차원 축소의 두 가지 주요 방법

. 특성 선택 (Feature Selection)
* 원본 데이터에서 가장 중요한 특성을 선택하여 차원을 줄임.
* 방법
** 상관관계 분석
** 피처 중요도 기반 선택 (예: 랜덤 포레스트 피처 중요도)
** 통계적 테스트
. 특성 추출 (Feature Extraction)
* 새로운 저차원 특성을 생성하여 데이터를 변환.
* 대표적인 방법:
** PCA: 공분산 행렬의 고유벡터를 사용해 새로운 축으로 데이터를 변환.
** t-SNE: 데이터의 고차원 구조를 저차원에서 비선형적으로 보존.
** LDA: 분류 작업에서 클래스 간 분산을 최대화하는 축으로 변환.

== 차원 축소 방법의 원리

.  PCA (Principal Component Analysis)
* 원리
.. 데이터의 공분산 행렬을 계산합니다.
.. 공분산 행렬의 고유값(eigenvalues)과 고유벡터(eigenvectors)를 구합니다.
.. 고유값이 큰 순서대로 고유벡터를 정렬하여 주요 축(Principal Components)을 생성합니다.
.. 데이터를 이 주요 축에 투영하여 저차원 데이터로 변환합니다.
* 핵심
.. 데이터의 최대 분산 방향을 찾고 그 방향으로 데이터를 투영.
.. 직교하는 새로운 축을 생성하여 중복 특성을 제거.
. t-SNE (t-Distributed Stochastic Neighbor Embedding)
* 원리
.. 고차원 공간에서 데이터 간의 유사성을 확률로 나타냅니다.
.. 저차원 공간에서도 유사성을 유지하도록 데이터 포인트를 배치합니다.
.. 확률 분포의 차이를 최소화하는 방식으로 반복적으로 최적화합니다.
* 핵심
.. 비선형 구조를 저차원에서 보존.
.. 특히, 클러스터링 구조를 시각화하는 데 유용.
. LDA (Linear Discriminant Analysis)
* 원리
.. 클래스 간 분산(클래스 간 평균 거리)을 최대화.
.. 클래스 내 분산(클래스 내부 데이터 흩어짐)을 최소화.
.. 이 두 목표를 동시에 달성하는 방향으로 데이터를 변환.
* 핵심
.. 분류 문제에 특화된 차원 축소 기법.

== 차원 축소의 한계

. 정보 손실:
* 차원을 줄이는 과정에서 일부 정보가 손실될 수 있음.
. 해석 어려움:
* 특성 추출 방식(PCA, t-SNE 등)으로 생성된 새로운 축은 원본 데이터의 의미를 잃을 수 있음.
. 비선형 데이터 한계:
* PCA는 선형 변환에 기반하므로, 비선형적인 데이터 구조는 효과적으로 처리하지 못함.

== 예제

PCA를 통한 차원 축소

[source, python]
----
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt

# 데이터 로드
data = load_iris()
X = data.data
y = data.target

# PCA 적용 (2차원으로 축소)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# 결과 시각화
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA: Iris Dataset')
plt.show()
----